{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690f6d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv('data.csv')\n",
    "print(\"Dataset size:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e61f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = df['NO2'].dropna().values.reshape(-1, 1)\n",
    "\n",
    "# Normalization is crucial for GAN stability\n",
    "x_mean, x_std = x.mean(), x.std()\n",
    "x_scaled = (x - x_mean) / x_std\n",
    "\n",
    "# Transformation x -> z\n",
    "r = 102483084\n",
    "ar = 0.5 * (r % 7)\n",
    "br = 0.3 * ((r % 5) + 1)\n",
    "\n",
    "z = x_scaled + ar * np.sin(br * x_scaled)\n",
    "z_tensor = torch.FloatTensor(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim=1, output_dim=1):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba81006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(z_tensor, epochs=10000, batch_size=128):\n",
    "    gen = Generator()\n",
    "    disc = Discriminator()\n",
    "    \n",
    "    g_optimizer = optim.Adam(gen.parameters(), lr=0.0002)\n",
    "    d_optimizer = optim.Adam(disc.parameters(), lr=0.0002)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(epochs):\n",
    "        # 1. Train Discriminator\n",
    "        disc.zero_grad()\n",
    "        \n",
    "        # Real samples\n",
    "        idx = torch.randint(0, len(z_tensor), (batch_size,))\n",
    "        real_data = z_tensor[idx]\n",
    "        real_labels = torch.ones(batch_size, 1)\n",
    "        \n",
    "        output_real = disc(real_data)\n",
    "        loss_real = criterion(output_real, real_labels)\n",
    "        \n",
    "        # Fake samples\n",
    "        noise = torch.randn(batch_size, 1)\n",
    "        fake_data = gen(noise)\n",
    "        fake_labels = torch.zeros(batch_size, 1)\n",
    "        \n",
    "        output_fake = disc(fake_data.detach())\n",
    "        loss_fake = criterion(output_fake, fake_labels)\n",
    "        \n",
    "        d_loss = loss_real + loss_fake\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # 2. Train Generator\n",
    "        gen.zero_grad()\n",
    "        output_gen = disc(fake_data)\n",
    "        # We want the discriminator to label these as '1' (real)\n",
    "        g_loss = criterion(output_gen, real_labels)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "            \n",
    "    return gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5145fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(z_real, gen_model):\n",
    "    gen_model.eval()\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(10000, 1)\n",
    "        z_fake = gen_model(noise).numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Kernel Density Estimation (KDE) for PDF comparison\n",
    "    sns.kdeplot(z_real.flatten(), label='True Distribution (z)', color='blue', shade=True)\n",
    "    sns.kdeplot(z_fake.flatten(), label='GAN Learned PDF (z_f)', color='red', linestyle='--')\n",
    "    \n",
    "    plt.title(\"PDF Approximation: GAN vs Real Data\")\n",
    "    plt.xlabel(\"Value of z\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e2a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    trained_gen = train_gan(z_tensor)\n",
    "    plot_results(z_tensor.numpy(), trained_gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
